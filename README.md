# Парсинг кник с сайта Amazon

### Процесс разбит на несколько этапов
1. Узнать id категорий книг на сайте amazon и добавить в список category.py. 
Для этого переходим в категорию на сайте -> https://www.amazon.com/s?bbn=1000&rh=n%3A283155%2Cn%3A18&dc&qid=1688637061&rnid=1000&ref=lp_1000_nr_n_19
видим *n%3A283155%2Cn%3A18&dc&*. Тут находятся id двух категорий, которые следую после *3A*(три А).
Берем второе значение(после второго 3А) - 18. Таким образом id = 18 у категории Mystery, Thriller & Suspense
2. Переходим https://www.amazon.com/s?i=stripbooks&rh=n%3A{номер_категории}&fs=true&page={страница}
и собираем ссылки на книги на всех страницах.
3. Переходим по собранным ссылкам и достаем необходимую информацию.

### Трудности
1. Капча! Капча наше все! Ну и периодически сайт не принемает куки.
#### Решение
Для обхода капчи пользовался динамическим прокси. Победить куки не удавалась таким образом. Поэтому использовал библиотеку 
Selenium. Можно после 500 запросов менять куки, включать и отключать куки - тоже работает периодически. Через 
selenium процесс медленней, поэтому запускал мультипроцессинг и натыкался на долгую загрузгу страницы.
Выставление по таймингу не помогало, отключене скриптов тоже. Для ускорения процесса можно просто нажимать на отсановку загрузки
страницы после прогрузки информации на сайте и все ок тогда. Естественно этот процесс можно автоматизировать.
Если клик будет раньше прогруженных данных. то парсер запустит эту страницу снова.

### Содержание
Файл clear_link.py используется для очистки лишних ссылок из списка книг(бывает при сборе ссылок проскакивает не на продукт)
Файл main.py и main_old.py используются для загрузки ссылок на продукты и запись их в файлы с именем названия категории.
Так же в этих файлах есть методы скачивания информации на продукт(requests и selenium).
Файл main_selenium_mult.py - это отдельный файл с использованием мультипроцессинга в Селениум. 
Количество окон будет равно количеству файлов с ссылками в папке books_url. Данные сохраняются в csv-файл в папку DATA.
ОСТОРОЖНО!!! При повторном запуске, если название файла из books_url совпадет с названием файла в DATA, то информация в csv
затрется. Поэтому спарсенную информацию переносите из папки DATA. можно комментить эти строки
```python
with open(path_data, 'w', ) as file_csv:
    writer = csv.DictWriter(file_csv, fieldnames=fields_to_save)
    writer.writeheader()
```
чтобы файл не перезаписывался, а писал дальше по строкам. Необходимо, если вы прервали процесс и хотите снова продолжить с сеста остановки.